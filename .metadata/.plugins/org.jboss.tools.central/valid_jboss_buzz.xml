<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Use vim in a production Red Hat OpenShift container in 6 easy steps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/USaqb-inbjk/" /><category term="Containers" /><category term="Kubernetes" /><category term="Linux" /><category term="Operating System" /><category term="Docker" /><category term="Fedora" /><category term="openshift" /><category term="Podman" /><category term="vim" /><author><name>Konrad Kleine</name></author><id>https://developers.redhat.com/blog/?p=806727</id><updated>2021-01-19T08:00:48Z</updated><published>2021-01-19T08:00:48Z</published><content type="html">&lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: In most cases, we don&amp;#8217;t recommend editing files in a container. However, in rare cases, you might need to reproduce and slightly modify a file in a production container, especially when debugging. (In this case, the &lt;a target="_blank" rel="nofollow" href="https://fedoraproject.org/wiki/Vim"&gt;vim&lt;/a&gt; method I&amp;#8217;m using works on Fedora 32 on my laptop and it is the base of my &lt;a target="_blank" rel="nofollow" href="http://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; container image.)&lt;/p&gt; &lt;p&gt;In this article, I present a quick demo on how to install and run vim in a production Red Hat OpenShift container, when vim was not installed in the &lt;a href="https://developers.redhat.com/topics/containers/"&gt;container&lt;/a&gt; image. I also describe the method to use to overcome an event where the local operating system and container base image diverge.&lt;/p&gt; &lt;h3&gt;Step 1: Copy the vim binary&lt;/h3&gt; &lt;p&gt;For &lt;code&gt;oc cp&lt;/code&gt; to work, copy the vim binary: &lt;code&gt;&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;$ cp /usr/bin/vim ~/Downloads/vim&lt;/pre&gt; &lt;p&gt;(This way of copying the vim binary works best for me, although there might be another cleaner way. Let me know in the comments if you have a different way of doing it.)&lt;/p&gt; &lt;h3&gt;Step 2: Log in to the &lt;code&gt;oc&lt;/code&gt; cluster&lt;/h3&gt; &lt;p&gt;To login to the &lt;code&gt;oc&lt;/code&gt; cluster, run the command:&lt;/p&gt; &lt;pre&gt;$ oc login ..&lt;/pre&gt; &lt;h3&gt;Step 3: Specify the container to install vim into&lt;/h3&gt; &lt;p&gt;I only have one container running in my pod, so oc picks the first container in the pod automatically:&lt;/p&gt; &lt;pre&gt;$ export POD=yourPodName&lt;/pre&gt; &lt;h3&gt;Step 4: Copy files needed to run vim&lt;/h3&gt; &lt;p&gt;For vim to start properly, copy this list of files. If vim doesn&amp;#8217;t start, add files to this list and copy them over:&lt;/p&gt; &lt;pre&gt;$ export VIM_DEPS="~/Downloads/vim /lib64/libgpm.so.2.1.0 /lib64/libpython3.8.so.1.0 /lib64/libgpm.so.2"&lt;/pre&gt; &lt;pre&gt;$ for i in $VIM_DEPS; do oc cp $i $POD:/home/worker; done&lt;/pre&gt; &lt;h3&gt;Step 5: Log into the pod&lt;/h3&gt; &lt;p&gt;To login to pod, run this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ oc rsh $POD&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Step 6: Run vim&lt;/h3&gt; &lt;p&gt;To run vim, enter the following:&lt;/p&gt; &lt;pre&gt;worker@pod$ LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/home/worker" PATH="$PATH:$PWD" vim&lt;/pre&gt; &lt;h3&gt;What if my operating system differs from the container&amp;#8217;s base image?&lt;/h3&gt; &lt;p style="text-align: left;"&gt;Make sure the architecture of your container image and laptop match. Then fire up a base image that can run locally in a container. Install vim in that container that runs on your localhost. Copy out the vim binary as is; for example, using &lt;code&gt;podman cp&lt;/code&gt; or &lt;code&gt;docker cp&lt;/code&gt;, and copy it to the pod as previously described. Run vim in the pod and observe what files are missing. These files can be taken from the container running on your localhost.&lt;/p&gt; &lt;p&gt;I hope that this quick tip vim helps when you need to reproduce and slightly modify a file in an OpenShift production container.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#38;linkname=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#38;linkname=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#38;linkname=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#38;linkname=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#38;linkname=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#38;linkname=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#38;linkname=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F19%2Fuse-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps%2F&amp;#038;title=Use%20vim%20in%20a%20production%20Red%20Hat%20OpenShift%20container%20in%206%20easy%20steps" data-a2a-url="https://developers.redhat.com/blog/2021/01/19/use-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps/" data-a2a-title="Use vim in a production Red Hat OpenShift container in 6 easy steps"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/19/use-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps/"&gt;Use vim in a production Red Hat OpenShift container in 6 easy steps&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/USaqb-inbjk" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Disclaimer: In most cases, we don&amp;#8217;t recommend editing files in a container. However, in rare cases, you might need to reproduce and slightly modify a file in a production container, especially when debugging. (In this case, the vim method I&amp;#8217;m using works on Fedora 32 on my laptop and it is the base of my [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/19/use-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps/"&gt;Use vim in a production Red Hat OpenShift container in 6 easy steps&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/01/19/use-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">806727</post-id><dc:creator>Konrad Kleine</dc:creator><dc:date>2021-01-19T08:00:48Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/01/19/use-vim-in-a-production-red-hat-openshift-container-in-6-easy-steps/</feedburner:origLink></entry><entry><title type="html">Keycloak 12.0.2 released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NitRTLxDU2Y/keycloak-1202-released.html" /><author><name /></author><id>https://www.keycloak.org//2021/01/keycloak-1202-released.html</id><updated>2021-01-19T00:00:00Z</updated><content type="html">To download the release go to . ALL RESOLVED ISSUES The full list of resolved issues are available in UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NitRTLxDU2Y" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://www.keycloak.org//2021/01/keycloak-1202-released.html</feedburner:origLink></entry><entry><title>Operator integration testing for Operator Lifecycle Manager</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JGPT-p5D_pY/" /><category term="Linux" /><category term="Mac" /><category term="Operator" /><category term="Red Hat CodeReady Containers" /><category term="Red Hat OpenShift Container Platform" /><author><name>Taneem Ibrahim</name></author><id>https://developers.redhat.com/blog/?p=784997</id><updated>2021-01-18T08:00:54Z</updated><published>2021-01-18T08:00:54Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Operators&lt;/a&gt; are one of the ways to package, deploy, and manage application distribution on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. After a developer creates an Operator, the next step is to get the Operator published on &lt;a target="_blank" rel="nofollow" href="https://operatorhub.io/"&gt;OperatorHub.io&lt;/a&gt;. Doing this allows users to install and deploy the Operator in their OpenShift clusters. The Operator is installed, updated, and the management lifecycle is handled by the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/operators/understanding_olm/olm-understanding-olm.html"&gt;Operator Lifecycle Manager (OLM)&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, we explore the steps required to test OLM integration for the Operator. For demonstration, we use a simple Operator that prints a test message to the shell. The Operator is packaged in the recently introduced &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/operators/olm-packaging-format.html#olm-bundle-format_olm-packaging-format"&gt;Bundle Format&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;span id="more-784997"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;For our local development environment, we need access to the following toolkits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/codeready-containers"&gt;Red Hat CodeReady Containers (CRC)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://podman.io/"&gt;Podman&lt;/a&gt;, or a Docker daemon process running on the local machine&lt;/li&gt; &lt;li&gt;Operator SDK toolkit, v1.0.0 or higher (optional)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/operator-framework/operator-registry/releases/tag/v1.13.8"&gt;Operator Package Manager (OPM)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;OpenShift Container Platform, cluster version 4.5 or higher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We need CRC because it provides convenient single-node minimal OpenShift clusters that are primarily intended to aid developers in testing. The OPM provides our local desktop environment.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: After downloading OPM, we rename the &lt;code&gt;opm&lt;/code&gt; binary for ease of use by setting the binary&amp;#8217;s permission to at least make it readable and executable.&lt;/p&gt; &lt;p&gt;Once you have everything set up, create a free account on &lt;a target="_blank" rel="nofollow" href="https://quay.io/"&gt;Red Hat Quay.io&lt;/a&gt;. We use Quay to build, analyze, distribute, and host container images. In this case, take note of the &lt;code&gt;quay_username&lt;/code&gt; and set the following environment variable for ease of use:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; export quay_username=&amp;#60;your_quay_user_name&amp;#62; &amp;#62; echo $quay_username&lt;/pre&gt; &lt;p&gt;Now we are ready to get started on our OLM integration test.&lt;/p&gt; &lt;h2&gt;Step 1: Download the Operator package&lt;/h2&gt; &lt;p&gt;To get started, let’s clone the following Git repository. This repository contains our example Operator bundle package that we deploy to the local OperatorHub instance in our cluster.&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62;  git clone https://github.com/taneem-ibrahim/olm-testing-bundle-format-index.git&lt;/pre&gt; &lt;p&gt;This creates the following directory structure:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;foo-operator % tree ├── bundle │   ├── manifests │   │   ├── example.com_foobars.yaml │   │   ├── foobar-operator-metrics-reader_rbac.authorization.k8s.io_v1beta1_clusterrole.yaml │   │   └── foobar-operator.clusterserviceversion.yaml │   ├── metadata │   │   └── annotations.yaml │   └── tests │       └── scorecard │           └── config.yaml ├── bundle.Dockerfile ├── catalogsource.yaml └── subscription.yaml&lt;/pre&gt; &lt;h2&gt;&lt;b&gt;Step 2: Build and push the Operator bundle image&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Our next step is to build the Operator bundle image and publish it to a container registry. In our examples, we use Quay to host our container images. From the Operator&amp;#8217;s root directory, &lt;code&gt;foobar-operator&lt;/code&gt;, let’s run the following commands. Substitute the &lt;code&gt;&amp;#60;quay_username&amp;#62;&lt;/code&gt; tag for your own &lt;code&gt;quay_username&lt;/code&gt;.&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: When you use Podman, you can run the following commands just by replacing the word Docker with Podman. For example:&lt;/p&gt;&lt;/blockquote&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; docker login quay.io -u $quay_username &amp;#62; docker build -f bundle.Dockerfile -t quay.io/$quay_username/foobar-operator:v0.0.1 . &amp;#62; docker push quay.io/$quay_username/foobar-operator:v0.0.1&lt;/pre&gt; &lt;p&gt;When we create a new repository in Quay by default, the repository visibility is set to private. For simplicity, we log into the Quay web portal and see the &lt;a target="_blank" rel="nofollow" href="https://docs.quay.io/guides/repo-view.html"&gt;repository access&lt;/a&gt; to the public. This can be set from the repository&amp;#8217;s Settings tab or by going to &lt;code&gt;https://quay.io/repository/&lt;/code&gt;. Then substitute the &lt;code&gt;quay_username&lt;/code&gt; or repository name accordingly.&lt;/p&gt; &lt;p&gt;When we prefer to keep the repository visibility private, we can add an &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets"&gt;image pull secret&lt;/a&gt;. Operator pods use the default &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/authentication/using-service-accounts-in-applications.html"&gt;service account&lt;/a&gt; in the &lt;code&gt;openshift-marketplace&lt;/code&gt; namespace.&lt;/p&gt; &lt;p&gt;This Operator is a simple image that loops and prints the following message &lt;code&gt;v0.0.1&lt;/code&gt; to the console:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;spec: ....               containers:               - &lt;b&gt;command: [ "/bin/sh", "-c", "while true ; do echo v0.0.1; sleep 10; done;" ]&lt;/b&gt;                 image: docker.io/busybox&lt;/pre&gt; &lt;h2&gt;&lt;b&gt;Step 3: Validate the Operator bundle package (optional)&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;This is an optional step. Let’s validate the Operator bundle package we just built and pushed. The following command needs to end with the message: &lt;code&gt;all validation tests have completed successfully&lt;/code&gt;&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; operator-sdk bundle validate quay.io/$quay_username/foobar-operator:v0.0.1&lt;/pre&gt; &lt;h2&gt;&lt;b&gt;Step 4: Build and push an index image&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Once the Operator bundle image is pushed, the next step is to create and publish an index image, making the Operator available using the OLM for users to install in their clusters. &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/operators/olm-managing-custom-catalogs.html#olm-creating-index-image_olm-managing-custom-catalogs"&gt;Index image&lt;/a&gt; is a database of pointers to Operator manifest content, which enables OLM to query the Operator image versions and get the desired Operator version installed on the cluster.&lt;/p&gt; &lt;p&gt;We use the OPM tool to create the index image for our &lt;code&gt;foobar-operator&lt;/code&gt; bundle. After building the image, we push the image to Quay. When we use Podman, we do not need to add &lt;code&gt;--build-tool docker&lt;/code&gt; because &lt;code&gt;opm&lt;/code&gt; defaults to Podman for the build tool:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; opm index add --bundles quay.io/$quay_username/foobar-operator:v0.0.1 --tag quay.io/$quay_username/foobar-operator-index:latest --build-tool docker &amp;#62; docker push quay.io/$quay_username/foobar-operator-index:latest&lt;/pre&gt; &lt;p&gt;As before, we need to set the repository visibility of &lt;code&gt;foobar-operator&lt;/code&gt; index to public for simplicity on Quay.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Step 5: Create a custom CatalogSource object&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/operators/olm-managing-custom-catalogs.html#olm-creating-catalog-from-index_olm-managing-custom-catalogs"&gt;CatalogSource&lt;/a&gt; represents Operator metadata that OLM can query to discover and install Operators and their dependencies. We create the following CatalogSource resource. Don’t forget to substitute the appropriate &lt;code&gt;quay_username&lt;/code&gt; according to the &lt;code&gt;$quay_username&lt;/code&gt; for the index image location on Quay. Save the file as &lt;code&gt;catalogsource.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;spec:   sourceType: grpc   image: quay.io/&lt;b&gt;&amp;#60;substitute_quay_username&amp;#62;&lt;/b&gt;/foobar-operator-index:latest   displayName: Custom Catalog   updateStrategy:     registryPoll:        interval: 5m&lt;/pre&gt; &lt;p&gt;Let’s create the catalog source object:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; oc create -f catalogsource.yaml&lt;/pre&gt; &lt;p&gt;We are using the &lt;code&gt;openshift-marketplace&lt;/code&gt; namespace above since it is a global namespace. This means a subscription created in any namespace is able to resolve in the cluster. However, we can choose any custom namespace here as long as it matches the related subscription object namespace created in the next step.&lt;/p&gt; &lt;p&gt;Additionally, the name of the CatalogSource object determines which catalog registry the Operator shows up under on the OperatorHub console. In the example above, we are using &lt;code&gt;custom&lt;/code&gt; with the display name set to CustomCatalog. We also configured the catalog to automatically poll for the latest version of the index image for the Operator every five minutes.&lt;/p&gt; &lt;p&gt;We can validate the deployment of the pod by querying the pod logs:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; oc get pods -n openshift-marketplace | grep “custom” &amp;#62; oc logs custom-&amp;#60;pod_id&amp;#62; -n openshift-marketplace &amp;#62; … level=info msg="serving registry" database=/database/index.db port=50051&lt;/pre&gt; &lt;h2&gt;&lt;b&gt;Step 6: Create a subscription object&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;A &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.5/operators/understanding_olm/olm-understanding-olm.html#olm-subscription_olm-understanding-olm"&gt;subscription&lt;/a&gt; is a custom resource that describes the channel the Operator subscribes to, and whether the Operator needs to be updated manually or automatically. We are installing the Operator in the alpha channel since our bundle manifest channel is set to &lt;code&gt;alpha&lt;/code&gt; in the annotations.yaml file. The other available channel is &lt;code&gt;stable&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We can validate the channel by running the following command:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; oc get packagemanifests foobar-operator -o jsonpath='{.status.defaultChannel}' &amp;#62; alpha&lt;/pre&gt; &lt;p&gt;The GitHub repository has a subscription.yaml file provided. From the &lt;code&gt;foobar-operator&lt;/code&gt; root directory, we can run the following command to create the subscription object:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; oc create -f subscription.yaml&lt;/pre&gt; &lt;h2&gt;&lt;b&gt;Step 7: Install the Operator from the OperatorHub&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Let’s log into the OpenShift admin console, navigate to the OperatorHub, and search for our Operator, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_785017" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/step-8.png"&gt;&lt;img aria-describedby="caption-attachment-785017" class="wp-image-785017 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/step-8-1024x269.png" alt="Operators -&amp;#62; OperatorHub -&amp;#62; Searching All Items for &amp;#34;foo&amp;#34; returns &amp;#34;foobar&amp;#34;" width="640" height="168" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/step-8-1024x269.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/step-8-300x79.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/step-8-768x202.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/step-8.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-785017" class="wp-caption-text"&gt;Figure 1. Use the OperatorHub to search and locate our Operator.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now we can install this Operator as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_785027" style="width: 242px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/step8-1.png"&gt;&lt;img aria-describedby="caption-attachment-785027" class="wp-image-785027 size-medium" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/step8-1-232x300.png" alt="foobar install section set to Capability Level Basic Install" width="232" height="300" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/step8-1-232x300.png 232w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/step8-1.png 628w" sizes="(max-width: 232px) 100vw, 232px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-785027" class="wp-caption-text"&gt;Figure 2. Click Install and select openshift-marketplace namespace.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We can also query the Operator pod logs to see if it’s printing &lt;code&gt;v0.0.1&lt;/code&gt; as we had in our deployment container spec for the Operator image:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; oc logs -f foobar-operator-controller-manager-&amp;#60;pod_id&amp;#62; -n openshift-marketplace &amp;#62; v0.0.1&lt;/pre&gt; &lt;p&gt;That’s it. We successfully validated our Operator integration with OLM.&lt;/p&gt; &lt;h2&gt;&lt;b&gt;Upgrade the Operator version&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Now we do a simple Operator upgrade test just by updating the Operator version tag from &lt;code&gt;0.0.1&lt;/code&gt; to &lt;code&gt;0.0.2&lt;/code&gt; in the CatalogService Version (CSV) file. Let’s run the following command from the root directory of our &lt;code&gt;foobar-operator&lt;/code&gt;:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; sed 's/0.0.1/0.0.2/g' ./bundle/manifests/foobar-operator.clusterserviceversion.yaml &amp;#62; ./bundle/manifests/foobar-operator.clusterserviceversion.yaml&lt;/pre&gt; &lt;p&gt;Next, we repeat Steps 1 and 2 above to build and optionally validate the new Operator bundle image, and substitute the image version tags to be &lt;code&gt;0.0.2&lt;/code&gt; instead of &lt;code&gt;0.0.1&lt;/code&gt; in the Docker push commands respectively.&lt;/p&gt; &lt;p&gt;We are now ready to add the new Operator version to the registry. We can do that by using the &lt;code&gt;opm add&lt;/code&gt; command. Notice how we are adding the upgraded Operator version cumulatively by inserting the &lt;code&gt;from-index&lt;/code&gt; parameter. If using Podman, then we do not have to pass the &lt;code&gt;--build-tool docker&lt;/code&gt; option since &lt;code&gt;opm&lt;/code&gt; defaults to Podman build tool:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; opm index add --bundles quay.io/$quay_username/foobar-operator:v0.0.2 --from-index quay.io/$quay_username/foobar-operator-index:latest --tag quay.io/$quay_username/foobar-operator-index:latest --build-tool docker&lt;/pre&gt; &lt;p&gt;In the Quay repository, we can validate the latest versions of the Operator image and index image by running the Docker images (or Podman images) command as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_785037" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-1.png"&gt;&lt;img aria-describedby="caption-attachment-785037" class="wp-image-785037 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-1-1024x54.png" alt="opm index add output showing two repositories, foobar-operator-index (set to latest) and foobar-operator (set to v0.0.2)" width="640" height="34" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-1-1024x54.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-1-300x16.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-1-768x40.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-1.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-785037" class="wp-caption-text"&gt;Figure 3. Validate the latest Operator images and index images using the Docker (or Podman) images command.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We push this new version of the index image to Quay:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; docker push quay.io/$quay_username/foobar-operator-index:latest&lt;/pre&gt; &lt;p&gt;The CatalogSource automatically polls for the latest version every five minutes. After five minutes have passed, we can go to the OperatorHub console and validate the Operator version 0.0.2 is available as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_785047" style="width: 310px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-2.png"&gt;&lt;img aria-describedby="caption-attachment-785047" class="wp-image-785047 size-medium" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-2-300x233.png" alt="The foobar Operator section showing version 0.0.2" width="300" height="233" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-2-300x233.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/upgrade-2.png 628w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-785047" class="wp-caption-text"&gt;Figure 4. Verify the OperatorHub console and validate that the available Operator version is 0.0.2.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Let’s install the new Operator version in the &lt;code&gt;openshift-marketplace&lt;/code&gt; namespace and validate the Operator pod log to see if &lt;code&gt;v0.0.2&lt;/code&gt; is being echoed:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;&amp;#62; oc logs -f foobar-operator-controller-manager-&amp;#60;pod_id&amp;#62; -n openshift-marketplace v0.0.2&lt;/pre&gt; &lt;p&gt;Now that you know how to test OLM integration for an Operator, give it a try with your own projects!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#38;linkname=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#38;linkname=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#38;linkname=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#38;linkname=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#38;linkname=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#38;linkname=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#38;linkname=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F18%2Foperator-integration-testing-for-operator-lifecycle-manager%2F&amp;#038;title=Operator%20integration%20testing%20for%20Operator%20Lifecycle%20Manager" data-a2a-url="https://developers.redhat.com/blog/2021/01/18/operator-integration-testing-for-operator-lifecycle-manager/" data-a2a-title="Operator integration testing for Operator Lifecycle Manager"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/18/operator-integration-testing-for-operator-lifecycle-manager/"&gt;Operator integration testing for Operator Lifecycle Manager&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JGPT-p5D_pY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Operators are one of the ways to package, deploy, and manage application distribution on Red Hat OpenShift. After a developer creates an Operator, the next step is to get the Operator published on OperatorHub.io. Doing this allows users to install and deploy the Operator in their OpenShift clusters. The Operator is installed, updated, and the [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/18/operator-integration-testing-for-operator-lifecycle-manager/"&gt;Operator integration testing for Operator Lifecycle Manager&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/01/18/operator-integration-testing-for-operator-lifecycle-manager/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">784997</post-id><dc:creator>Taneem Ibrahim</dc:creator><dc:date>2021-01-18T08:00:54Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/01/18/operator-integration-testing-for-operator-lifecycle-manager/</feedburner:origLink></entry><entry><title>Coming in glibc 2.33: Reloadable nsswitch.conf</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/icXSyt_kNkE/" /><category term="C" /><category term="Linux" /><category term="glibc" /><category term="nsswitch" /><category term="nsswitch.conf" /><category term="reload config" /><author><name>DJ Delorie</name></author><id>https://developers.redhat.com/blog/?p=835617</id><updated>2021-01-15T08:00:59Z</updated><published>2021-01-15T08:00:59Z</published><content type="html">&lt;p&gt;In my &lt;a href="https://developers.redhat.com/blog/2018/11/26/etc-nsswitch-conf-non-complexity/"&gt;previous article about nsswitch.conf&lt;/a&gt; I talked about how simple, perhaps too simple, this config file is to use. What I didn&amp;#8217;t cover then was how simplistic its internal implementation is. Specifically, an application only loads this file once—the first time it&amp;#8217;s needed.&lt;/p&gt; &lt;p&gt;So, what do you do when &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Name_Service_Switch"&gt;nsswitch.conf&lt;/a&gt; needs to change? How do you update all of the running applications? You don’t! The only way to force a reload is to stop the application and restart it. That is not always an option, especially for critical applications that might take a long time to restart.&lt;/p&gt; &lt;p&gt;Recent work behind the scenes in the GNU C library will change all of this. As of &lt;a target="_blank" rel="nofollow" href="https://www.gnu.org/software/libc/"&gt;glibc&lt;/a&gt; version 2.33, this config file now reloads and reparses each time it changes, and only the configuration is reloaded. If the configuration calls for an external shared library to be loaded, that object is only ever loaded once. It may be called in a different sequence, or not called at all, but it is never unloaded. This behavior avoids a whole class of problems related to unloading shared objects that might still be in use.&lt;/p&gt; &lt;p&gt;Most applications will never know any of this is happening. They do their lookups and get the data they need, even if it’s different than the last time. Applications that cache their lookups will never know anything changed. The catch is that if an application caches some of its lookups, but not others, it might receive an inconsistent set of information. Applications should already accommodate changes in the data, such as host addresses or the occasional UID update, so changing how that information is provided should not significantly change the application or increase the burden to the programmer.&lt;/p&gt; &lt;p&gt;I’ll address one concern likely to come up—memory. To avoid unrestrained growth in long-running programs, the parser maintains a pool of the pre-parsed lines it’s seen, and an array of the services it provides. It only needs to link the existing bits of data together. However, this practice assumes that the overall number of action permutations is limited. Most systems cycle between a few lines, so the size of this pool is limited, as shown in Figure 1. The data for each shared object is also fixed once the object is loaded.&lt;/p&gt; &lt;p&gt;&lt;div id="attachment_835637" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/12/data.png"&gt;&lt;img aria-describedby="caption-attachment-835637" class="wp-image-835637" src="https://developers.redhat.com/blog/wp-content/uploads/2020/12/data.png" alt="Change flow: passwd&amp;#62; dns files &amp;#62; libnss_dns.so + libnss_files.so + __nss_files, group &amp;#62; files, hosts &amp;#62; dns [NOTFOUND=return] files" width="640" height="253" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/12/data.png 764w, https://developers.redhat.com/blog/wp-content/uploads/2020/12/data-300x119.png 300w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-835637" class="wp-caption-text"&gt;Figure 1: Create the data for each shared object that is fixed once the object is loaded.&lt;/p&gt;&lt;/div&gt;Of course, there is one key caveat. Anything that updates nsswitch.conf needs to do it as atomically as possible so that applications don’t see a partial configuration and try to load it. If you are using a tool like &lt;code&gt;rsync&lt;/code&gt; to update remote machines (say, in a cluster or compute farm, or across a business unit), make sure you don’t use the &lt;code&gt;--inplace&lt;/code&gt; option. You might want to use a create/copy/rename sequence so that glibc doesn’t see a half-copied file.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#38;linkname=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#38;linkname=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#38;linkname=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#38;linkname=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#38;linkname=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#38;linkname=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#38;linkname=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2Fcoming-in-glibc-2-33-reloadable-nsswitch-conf%2F&amp;#038;title=Coming%20in%20glibc%202.33%3A%20Reloadable%20nsswitch.conf" data-a2a-url="https://developers.redhat.com/blog/2021/01/15/coming-in-glibc-2-33-reloadable-nsswitch-conf/" data-a2a-title="Coming in glibc 2.33: Reloadable nsswitch.conf"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/15/coming-in-glibc-2-33-reloadable-nsswitch-conf/"&gt;Coming in glibc 2.33: Reloadable nsswitch.conf&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/icXSyt_kNkE" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In my previous article about nsswitch.conf I talked about how simple, perhaps too simple, this config file is to use. What I didn&amp;#8217;t cover then was how simplistic its internal implementation is. Specifically, an application only loads this file once—the first time it&amp;#8217;s needed. So, what do you do when nsswitch.conf needs to change? How [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/15/coming-in-glibc-2-33-reloadable-nsswitch-conf/"&gt;Coming in glibc 2.33: Reloadable nsswitch.conf&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/01/15/coming-in-glibc-2-33-reloadable-nsswitch-conf/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">835617</post-id><dc:creator>DJ Delorie</dc:creator><dc:date>2021-01-15T08:00:59Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/01/15/coming-in-glibc-2-33-reloadable-nsswitch-conf/</feedburner:origLink></entry><entry><title>10 reasons to develop Quarkus applications on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/MsGdXON1s4c/" /><category term="Java" /><category term="Knative" /><category term="Kubernetes" /><category term="Quarkus" /><category term="Serverless" /><category term="MicroProfile" /><category term="openshift" /><category term="OpenTracing" /><category term="Spring framework" /><author><name>jebeck</name></author><id>https://developers.redhat.com/blog/?p=851277</id><updated>2021-01-15T08:00:20Z</updated><published>2021-01-15T08:00:20Z</published><content type="html">&lt;p&gt;Combining &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; with &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; provides an ideal environment for creating scalable, fast, and lightweight applications. Quarkus significantly increases developer productivity with tooling, pre-built integrations, application services, and more. This article presents 10 reasons why you should develop your Quarkus applications on OpenShift.&lt;/p&gt; &lt;h2&gt;Reason 1: One-step OpenShift deployment&lt;/h2&gt; &lt;p&gt;You don’t have to be an OpenShift expert to deploy Quarkus applications. The Quarkus OpenShift extension automatically generates OpenShift resources, making it easy to get started. The extension provides multiple deployment options, including &lt;a target="_blank" rel="nofollow" href="https://github.com/GoogleContainerTools/jib"&gt;Jib&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/container-image#docker"&gt;Docker&lt;/a&gt;, and Source-to-Image (&lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/enterprise/3.0/creating_images/s2i.html#:~:text=Source%2Dto%2DImage%20(S2I,ease%20of%20use%20for%20developers."&gt;S2i&lt;/a&gt;). It also creates a &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.6/applications/deployments/what-deployments-are.html"&gt;DeploymentConfig&lt;/a&gt;, which triggers an automatic redeployment whenever a change is detected in the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.6/rest_api/image_apis/imagestream-image-openshift-io-v1.html"&gt;ImageStream&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here is a simple example of a Quarkus deployment on OpenShift:&lt;/p&gt; &lt;pre&gt;//deploy JVM-based app on OpenShift //add OpenShift extension mvn quarkus:add-extension -Dextensions="openshift" //application.properties quarkus.s2i.base-jvm-image= quarkus.openshift.expose=true mvn clean package -Dquarkus.kubernetes.deploy=true &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/deploying-to-openshift"&gt;Deploying Quarkus on OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 2: One-step serverless function deployment&lt;/h2&gt; &lt;p&gt;Quarkus applications, especially those compiled to native code, are ideal for serverless applications due to their small size and fast boot times. The Quarkus OpenShift extension also makes it easy to deploy and scale &lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/learn/topics/serverless"&gt;Knative serverless services&lt;/a&gt;. As a developer, you don’t need to worry about server provisioning or maintaining the underlying infrastructure. You simply write your code and package it in a container for deployment.&lt;/p&gt; &lt;p&gt;Here is an example of a serverless function deployment:&lt;/p&gt; &lt;pre&gt;//deploy serverless knative app on OpenShift //add OpenShift extension mvn quarkus:add-extension -Dextensions="openshift" //application.propertiesquarkus.s2i.base-jvm-image= quarkus.kubernetes.deployment-target=knative mvn clean package -Dquarkus.kubernetes.deploy=true &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/deploying-to-openshift#knative-openshift-serverless"&gt;Using Knative via OpenShift Serverless&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 3: Live coding&lt;/h2&gt; &lt;p&gt;The traditional Java development workflow is a major drain on productivity. It can take minutes to complete each iteration in a cycle. The Quarkus live coding feature solves this problem. Figure 1 illustrates an example workflow when running in development mode.&lt;/p&gt; &lt;div id="attachment_852997" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-docs.google.com-2021.01.08-14_18_48.png"&gt;&lt;img aria-describedby="caption-attachment-852997" class="wp-image-852997" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-docs.google.com-2021.01.08-14_18_48.png" alt="write code + compile + deploy + refresh browser + repeat" width="640" height="84" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-docs.google.com-2021.01.08-14_18_48.png 932w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-docs.google.com-2021.01.08-14_18_48-300x39.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-docs.google.com-2021.01.08-14_18_48-768x101.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-852997" class="wp-caption-text"&gt;Figure 1: The write and refresh development cycle in Quarkus.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Here&amp;#8217;s the command to run Quarkus in development mode:&lt;/p&gt; &lt;pre&gt;mvn compile quarkus:dev &lt;/pre&gt; &lt;p&gt;Given this command, Quarkus checks to see if any application source files have changed. If they have, Quarkus transparently compiles the changed files and redeploys the application.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/vision/developer-joy#live-coding"&gt;Live coding with Quarkus&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 4: Remote development and debugging&lt;/h2&gt; &lt;p&gt;You can also do live coding remotely, in development mode, in a clustered OpenShift or Kubernetes environment. Any changes you make locally will be immediately visible in the clustered environment. Remote development and debugging lets you create applications in the same environment where your applications will run. The key is building a mutable application:&lt;/p&gt; &lt;pre&gt;//application.properties quarkus.package.type=mutable-jar quarkus.live-reload.password=abc123 quarkus.kubernetes.env.vars.QUARKUS_LAUNCH_DEVMODE=true //Deploy mvn clean install -Dquarkus.kubernetes.deploy=true //Start mvnw quarkus:remote-dev -Dquarkus.live-reload.url= &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/maven-tooling#remote-development-mode"&gt;Building Quarkus applications in remote development mode&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 5: Access to OpenShift ConfigMaps and secrets&lt;/h2&gt; &lt;p&gt;Quarkus includes a &lt;code&gt;kubernetes-config&lt;/code&gt; extension that lets you use Kubernetes &lt;code&gt;ConfigMap&lt;/code&gt;s and secrets as a configuration source. You never even have to mount them into the pod running your Quarkus application. Instead, your Quarkus application reads &lt;code&gt;ConfigMap&lt;/code&gt;s and secrets directly from the Kubernetes API server using the Kubernetes client:&lt;/p&gt; &lt;pre&gt;//application.properties quarkus.kubernetes-config.enabled=true quarkus.kubernetes-config.secrets.enabled=true quarkus.kubernetes-config.config-maps= mvn clean package -Dquarkus.kubernetes.deploy=true &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/kubernetes-config"&gt;The Quarkus kubernetes-config extension&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 6: Health endpoints&lt;/h2&gt; &lt;p&gt;Quarkus uses the &lt;a target="_blank" rel="nofollow" href="https://download.eclipse.org/microprofile/microprofile-health-2.1/microprofile-health-spec.html"&gt;MicroProfile Health specification&lt;/a&gt; (via the SmallRye extension) to provide information about the application state, such as availability and status. This information is useful in cloud environments where automated processes must frequently determine whether to discard or restart an application. Most Quarkus client extensions have built-in health status enabled by default:&lt;/p&gt; &lt;pre&gt;//add Kubernetes-config extension mvn quarkus:add-extension -Dextensions="smallrye-health" //validate health extension mvnw compile quarkus:dev curl http://localhost:8080/health/live //org.acme.microprofile.health.SimpleHealthCheck class package org.acme.microprofile.health; import org.eclipse.microprofile.health.HealthCheck; import org.eclipse.microprofile.health.HealthCheckResponse; import org.eclipse.microprofile.health.Liveness; import javax.enterprise.context.ApplicationScoped; @Liveness @ApplicationScoped public class SimpleHealthCheck implements HealthCheck { @Override public HealthCheckResponse call() { return HealthCheckResponse.up("Simple health check"); } } &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/microprofile-health"&gt;Using the MicroProfile Health specification in Quarkus&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 7: Application metrics support&lt;/h2&gt; &lt;p&gt;Quarkus uses the Micrometer extension to support capturing runtime and application metrics. These metrics provide insight into what is happening inside the application. You can also format Micrometer extension metrics for processing with tools like &lt;a target="_blank" rel="nofollow" href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://grafana.com/"&gt;Grafana&lt;/a&gt;, which support analysis and visualization:&lt;/p&gt; &lt;pre&gt;//add Kubernetes-config extension mvn quarkus:add-extension -Dextensions="micrometer" //validate health extension mvnw compile quarkus:dev //Code snippet to discover, count, store, and record prime numbers @Path("/") public class PrimeNumberResource { private final LongAccumulator highestPrime = new LongAccumulator(Long::max, 0); private final MeterRegistry registry; PrimeNumberResource(MeterRegistry registry) { this.registry = registry; // Create a gauge to obtain the highest observed prime number registry.gauge("prime.number.max", this,PrimeNumberResource::highestObservedPrimeNumber);} // Return the highest observed prime value long highestObservedPrimeNumber() { return highestPrime.get();} } &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/micrometer#support-for-the-microprofile-metrics-api"&gt;The MicroProfile Metrics API in Quarkus&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 8: Tracing support&lt;/h2&gt; &lt;p&gt;Quarkus uses the &lt;a target="_blank" rel="nofollow" href="https://download.eclipse.org/microprofile/microprofile-3.3/microprofile-spec-3.3.html#mp-opentracing"&gt;MicroProfile OpenTracing&lt;/a&gt; specification (via the SmallRye extension) to provide distributed tracing across services for interactive web applications. The SmallRye extension includes the default &lt;a target="_blank" rel="nofollow" href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt; tracer to monitor and troubleshoot transactions in a distributed system:&lt;/p&gt; &lt;pre&gt;//add Kubernetes-config extension mvn quarkus:add-extension -Dextensions="smallrye-opentracing" //validate health extension mvnw compile quarkus:dev //REST endpoints are automatically traced. Here's how to trace additional methods import javax.enterprise.context.ApplicationScoped; import org.eclipse.microprofile.opentracing.Traced; @Traced @ApplicationScoped public class FrancophoneService { public String bonjour() { return "bonjour";} } &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/opentracing"&gt;Using OpenTracing in Quarkus applications&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reason 9: Developer tooling&lt;/h2&gt; &lt;p&gt;You might come for the performance, but you&amp;#8217;ll stay for the developer productivity. Developer tooling makes it even easier to develop and deploy Quarkus applications on OpenShift. Here are a few examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;IDE support&lt;/b&gt;: Quarkus utilizes the &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/quarkus-ls"&gt;Quarkus Language Server&lt;/a&gt; to support your favorite IDE, including &lt;a href="https://developers.redhat.com/blog/category/vs-code/"&gt;VSCode&lt;/a&gt;, Eclipse, IntelliJ, and &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/blog/march-of-ides/"&gt;more&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Build tools&lt;/b&gt;: Quarkus also supports &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/maven-tooling"&gt;Maven&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/gradle-tooling"&gt;Gradle&lt;/a&gt; build tools.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Codestarts&lt;/b&gt;: Extension &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/blog/extension-codestarts-a-new-way-to-learn-and-discover-quarkus/"&gt;codestarts&lt;/a&gt; include code examples and documentation to make it easier for developers new to Quarkus to create applications.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Reason 10: Compatibility with Spring APIs&lt;/h2&gt; &lt;p&gt;Spring is a dominant Java framework for developers, but Spring applications were not designed for cloud-native environments like OpenShift. Quarkus, on the other hand, was created and optimized for the cloud. As a result, Quarkus can &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/resources/mi-quarkus-lab-validation-idc-analyst-paper"&gt;reduce cloud-resource efficiency by up to 64%&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you need cloud-native efficiency but prefer to stick with the framework you know, Quarkus provides a Spring-compatibility layer. This means you can create applications using the Spring APIs you are familiar with, including data, web, config, security, dependency injection, and more. Here&amp;#8217;s an example of Spring web development in Quarkus:&lt;/p&gt; &lt;pre&gt;//Spring Web example import java.util.List; import org.springframework.web.bind.annotation.*; @RestController @RequestMapping("/person") public class PersonController { @GetMapping(path = "/greet/{id}", produces = "text/plain") public String greetPerson(@PathVariable(name = "id") long id) { String name=""; return name; } @GetMapping(produces = "application/json") public Iterable findAll() { return personRepository.findAll(); } &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Learn more&lt;/b&gt;: &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/blog/quarkus-for-spring-developers/"&gt;Quarkus for Spring developers&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Get started with Quarkus&lt;/h2&gt; &lt;p&gt;I hope the availability of developer tooling, pre-built integrations, and application services inspires you to develop your first Quarkus application on OpenShift. These additional resources will help you get started:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;Interactive tutorials&lt;/b&gt;: The Quarkus homepage includes numerous &lt;a target="_blank" rel="nofollow" href="https://learn.openshift.com/developing-with-quarkus/"&gt;interactive tutorials&lt;/a&gt; that walk you through building Quarkus applications in a pre-configured OpenShift environment.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Generate a Quarkus project&lt;/b&gt;: Quarkus project initializers make it easy to select extensions and generate sample applications for both the &lt;a target="_blank" rel="nofollow" href="http://code.quarkus.io"&gt;community&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://code.quarkus.redhat.com/"&gt;Red Hat&lt;/a&gt; builds of Quarkus.&lt;/li&gt; &lt;li&gt;&lt;b&gt;OpenShift access&lt;/b&gt;: Red Hat provides several options for accessing an OpenShift environment, including the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;developer sandbox&lt;/a&gt; shown in Figure 2. &lt;div id="attachment_853397" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-console-openshift-console.apps_.sandbox.x8i5.p1.openshiftapps.com-2021.01.08-16_41_51.png"&gt;&lt;img aria-describedby="caption-attachment-853397" class="wp-image-853397 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-console-openshift-console.apps_.sandbox.x8i5.p1.openshiftapps.com-2021.01.08-16_41_51-1024x574.png" alt="The sandbox include quick starts, samples, and a variety of deployment options." width="640" height="359" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-console-openshift-console.apps_.sandbox.x8i5.p1.openshiftapps.com-2021.01.08-16_41_51-1024x574.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-console-openshift-console.apps_.sandbox.x8i5.p1.openshiftapps.com-2021.01.08-16_41_51-300x168.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-console-openshift-console.apps_.sandbox.x8i5.p1.openshiftapps.com-2021.01.08-16_41_51-768x431.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/01/screenshot-console-openshift-console.apps_.sandbox.x8i5.p1.openshiftapps.com-2021.01.08-16_41_51.png 1566w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-853397" class="wp-caption-text"&gt;Figure 2: The OpenShift developer sandbox.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/try?extIdCarryOver=true&amp;#38;sc_cid=701f2000001OH74AAG"&gt;Learn more about the possibilities&lt;/a&gt; of using a Red Hat OpenShift 4 cluster on your computer, in your datacenter, and more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#38;linkname=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#38;linkname=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#38;linkname=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#38;linkname=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#38;linkname=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#38;linkname=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#38;linkname=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F15%2F10-reasons-to-develop-quarkus-applications-on-red-hat-openshift%2F&amp;#038;title=10%20reasons%20to%20develop%20Quarkus%20applications%20on%20Red%20Hat%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2021/01/15/10-reasons-to-develop-quarkus-applications-on-red-hat-openshift/" data-a2a-title="10 reasons to develop Quarkus applications on Red Hat OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/15/10-reasons-to-develop-quarkus-applications-on-red-hat-openshift/"&gt;10 reasons to develop Quarkus applications on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/MsGdXON1s4c" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Combining Quarkus with Red Hat OpenShift provides an ideal environment for creating scalable, fast, and lightweight applications. Quarkus significantly increases developer productivity with tooling, pre-built integrations, application services, and more. This article presents 10 reasons why you should develop your Quarkus applications on OpenShift. Reason 1: One-step OpenShift deployment You don’t have to be an [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/15/10-reasons-to-develop-quarkus-applications-on-red-hat-openshift/"&gt;10 reasons to develop Quarkus applications on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/01/15/10-reasons-to-develop-quarkus-applications-on-red-hat-openshift/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">851277</post-id><dc:creator>jebeck</dc:creator><dc:date>2021-01-15T08:00:20Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/01/15/10-reasons-to-develop-quarkus-applications-on-red-hat-openshift/</feedburner:origLink></entry><entry><title type="html">WildFly Bootable JAR 3.0 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/T8bqIWEwfvM/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/01/15/bootable-jar-3.0.Final-Released/</id><updated>2021-01-15T00:00:00Z</updated><content type="html">The version 3.0 of the has been released. For people who are not familiar with WildFly Bootable JAR, I strongly recommend that you read this that covers it in detail. The new features that come in this release (in particular the dev-watch goal) have already been covered when the Beta1 release. The Bootable JAR contains a new that provides details on the dev-watch goal. Since Beta1, the Bootable JAR have been evolved to use WildFly 22.0.0.Final that has just been . In this blog post I will focus on the integration with . This integration is now possible thanks to the Keycloak Galleon feature-pack that is new in . KEYCLOAK GALLEON FEATURE-PACK The provides an OIDC client adapter by the means of a Galleon layer. The keycloak-client-oidc Galleon layer brings the OIDC keycloak subsystem (and associated JBoss modules) and configures the server security. For more information on the Keycloak Galleon feature-pack usage with WildFly, you can read the in the WildFly documentation. Using the Bootable JAR Maven plugin, you can combine layers coming from multiple Galleon feature-packs. In your plugin configuration it looks like: &lt;configuration&gt; &lt;feature-packs&gt; &lt;feature-pack&gt; &lt;location&gt;wildfly@maven(org.jboss.universe:community-universe)&lt;/location&gt; &lt;/feature-pack&gt; &lt;feature-pack&gt; &lt;location&gt;org.keycloak:keycloak-adapter-galleon-pack:12.0.1&lt;/location&gt; &lt;/feature-pack&gt; &lt;/feature-packs&gt; &lt;layers&gt; &lt;layer&gt;web-server&lt;/layer&gt; &lt;layer&gt;keycloak-client-oidc&lt;/layer&gt; &lt;/layers&gt; ... &lt;/configuration&gt; You will notice that the &lt;location&gt; element is structured differently for the WildFly feature-pack and the Keycloak one. The WildFly feature-pack is accessible from the Galleon community-universe. Retrieving the WildFly feature-pack from this universe allows us to not specify any version and get the latest one. Since the Keycloak Galleon feature-pack is not registered inside an universe, you need to specify its Maven coordinates. When building a Bootable JAR, the plugin retrieves the feature-packs from Maven repositories and provisions a server based on the set of Galleon layers. You can follow the steps documented in this to secure a servlet deployed inside a Bootable JAR using Keycloak. DEVELOPING ON OPENSHIFT WITH BOOTABLE JAR We are currently working at defining dev files in order to make for a smooth and efficient development experience on OpenShift. Changes done in your local Maven project fire automatic re-build and re-deploy of your application in the remote OpenShift cluster POD. To achieve this, we are leveraging the ODO source watching capabilities to push changes onto your OpenShift POD. dev-watch goal, that is running in the POD, takes over and automatically re-build/re-deploy your application. Stay tuned! KNOWN ISSUES We have encountered an issue when using the dev-watch goal with the . You need to upgrade the plugin to 3.0.2.Final to allow for dev-watch to properly operate. The Maven command to use is: mvn wildfly-jar:dev-watch -Pbootable-jar -Dversion.wildfly-jar.maven.plugin=3.0.2.Final TO CONCLUDE If you have an interesting use-case, simple enough to be reduced to a simple example, and it is not covered by the , then feel free to one. We will be very happy to help you integrate your example in the project. Just ask! Finally we would really appreciate that you keep us posted with your feedback and new requirements (that you can log as new ), this will help evolve the WildFly Bootable JAR experience in the right direction. Thank-you! JF Denise&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/T8bqIWEwfvM" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/01/15/bootable-jar-3.0.Final-Released/</feedburner:origLink></entry><entry><title type="html">RESTEasy 4.6.0.Final is now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Q3GjepZlze4/" /><author><name /></author><id>https://resteasy.github.io/2021/01/14/resteasy-4.6.0.Final/</id><updated>2021-01-14T19:49:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Q3GjepZlze4" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/01/14/resteasy-4.6.0.Final/</feedburner:origLink></entry><entry><title>Knowledge meets machine learning for smarter decisions, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bmbDwQ0PJ4c/" /><category term="Big Data" /><category term="Machine Learning" /><category term="Python" /><category term="AI/ML" /><category term="business process automation" /><category term="Drools" /><category term="knowledge engineering" /><category term="machine learning model" /><category term="PMML" /><author><name>Donato Marrazzo</name></author><id>https://developers.redhat.com/blog/?p=815627</id><updated>2021-01-14T08:00:15Z</updated><published>2021-01-14T08:00:15Z</published><content type="html">&lt;p&gt;Drools is a popular open source project known for its powerful rules engine. Few users realize that it can also be a gateway to the amazing possibilities of &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence&lt;/a&gt;. This two-part article introduces you to using &lt;a href="https://developers.redhat.com/products/red-hat-decision-manager/overview"&gt;Red Hat Decision Manager&lt;/a&gt; and its Drools-based rules engine to combine &lt;a href="https://developers.redhat.com/blog/category/machine-learning/"&gt;machine learning&lt;/a&gt; predictions with deterministic reasoning. In Part 1, we&amp;#8217;ll prepare our machine learning logic. In Part 2, you&amp;#8217;ll learn how to use the machine learning model from a knowledge service.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: Examples in this article are based on Red Hat Decision Manager, but all of the technologies used are open source.&lt;/p&gt; &lt;h2&gt;Machine learning meets knowledge engineering&lt;/h2&gt; &lt;p&gt;Few Red Hat Decision Manager users know about its roots in artificial intelligence (AI), specifically the AI branch of knowledge engineering (also known as knowledge representation and reasoning). This branch aims to solve the problem of how to organize human knowledge so that a computer can treat it. Knowledge engineering uses &lt;i&gt;business rules&lt;/i&gt;, which means a set of knowledge metaphors that subject matter experts can easily understand and use.&lt;/p&gt; &lt;p&gt;The &lt;a target="_blank" rel="nofollow" href="https://www.omg.org/dmn/"&gt;Decision Model and Notation&lt;/a&gt; (DMN) standard recently released a new model and notation for subject matter experts. After years of using different methodologies and tools, we finally have a common language for sharing knowledge representation. A hidden treasure of the DMN is that it makes dealing with machine learning algorithms easier. The connecting link is another well-known standard in data science: The &lt;a target="_blank" rel="nofollow" href="http://dmg.org/pmml/v4-1/GeneralStructure.html"&gt;Predictive Model Markup Language&lt;/a&gt;, or PMML.&lt;/p&gt; &lt;p&gt;Using these tools to connect knowledge engineering and machine learning empowers both domains, so that the whole is greater than the sum of its parts. It opens up a wide range of use cases where combining deterministic knowledge and data science predictions leads to smarter decisions.&lt;/p&gt; &lt;h2&gt;A use case for cooperation&lt;/h2&gt; &lt;p&gt;The idea of algorithms that can learn from large sets of data and understand patterns that we humans cannot see is fascinating. However, overconfidence in machine learning technology leads us to underestimate the value of human knowledge.&lt;/p&gt; &lt;p&gt;Let’s take an example from our daily experience: We are all used to algorithms that use our internet browsing history to show us ads for products we&amp;#8217;ve already purchased. This happens because it’s quite difficult to train a machine learning algorithm to exclude ads for previously purchased products.&lt;/p&gt; &lt;p&gt;What is a difficult problem for machine learning is very easy for knowledge engineering to solve. On the flip side, encoding all possible relationships between searched words and suggested products is extremely tedious. In this realm, machine learning complements knowledge engineering.&lt;/p&gt; &lt;p&gt;Artificial intelligence has many branches—machine learning, knowledge engineering, search optimization, natural language processing, and more. Why not use more than one technique to achieve more intelligent behavior?&lt;/p&gt; &lt;h2&gt;Artificial intelligence, machine learning, and data science&lt;/h2&gt; &lt;p&gt;Artificial intelligence, machine learning, and data science are often used interchangeably. Actually, they are different but overlapping domains. As I already noted, artificial intelligence has a broader scope than machine learning. Machine learning is just one facet of artificial intelligence. Similarly, some argue that data science is a facet of artificial intelligence. Others say the opposite, that data science includes AI.&lt;/p&gt; &lt;p&gt;In the field, data scientists and AI experts offer different kinds of expertise with some overlap. Data science uses many machine learning algorithms, but not all of them. The Venn diagram in Figure 1 shows the spaces where artificial intelligence, machine learning, and data science overlap.&lt;/p&gt; &lt;div id="attachment_815637" style="width: 582px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25e53ede08.png"&gt;&lt;img aria-describedby="caption-attachment-815637" class="wp-image-815637 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25e53ede08.png" alt="Artificial intelligence and data science overlap. Machine learning is a subset of artificial intelligence that overlaps with data science." width="572" height="364" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25e53ede08.png 572w, https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25e53ede08-300x191.png 300w" sizes="(max-width: 572px) 100vw, 572px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-815637" class="wp-caption-text"&gt;Figure 1: The overlaps between artificial intelligence, machine learning, and data science.&lt;/p&gt;&lt;/div&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: See &lt;a target="_blank" rel="nofollow" href="https://www.mygreatlearning.com/blog/difference-data-science-machine-learning-ai/"&gt;Data Science vs. Machine Learning and Artificial Intelligence&lt;/a&gt; for more about each of these technology domains and the spaces where they meet.&lt;/p&gt; &lt;h2&gt;Craft your own machine learning model&lt;/h2&gt; &lt;p&gt;Data scientists are in charge of defining machine learning models after careful preparation. This section will look at some of the techniques data scientists use to select and tune a machine learning algorithm. The goal is to understand the workflow and learn how to craft a model that can cope with prediction problems.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: To learn more about data science methods and processes, see Wikipedia&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://en.m.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining"&gt;Cross-industry standard process for data mining&lt;/a&gt; (CRISP-DM) page.&lt;/p&gt; &lt;h3&gt;Prepare and train a machine learning algorithm&lt;/h3&gt; &lt;p&gt;The first step for preparing and training a machine learning algorithm is to collect, analyze, and clean the data that we will use. Data preparation is an important phase that significantly impacts the quality of the final outcome. Data scientists use mathematics and statistics for this phase.&lt;/p&gt; &lt;p&gt;For simplicity, let’s say we have a reliable data set based on a manager’s historical decisions in an order-fulfillment process. The manager receives the following information: Product type (examples are phone, printer, and so on), price, urgency, and category. There are two categories: &lt;i&gt;Basic&lt;/i&gt;, for when the product is required employee equipment, and &lt;i&gt;optional&lt;/i&gt;, for when the product is not necessary for the role.&lt;/p&gt; &lt;p&gt;The two decision outcomes are &lt;i&gt;approved&lt;/i&gt; or &lt;i&gt;denied&lt;/i&gt;. Automating this decision will free the manager from a repetitive task and speed up the overall order-fulfillment process.&lt;/p&gt; &lt;p&gt;As a first attempt, we could take the data as-is to train the model. Instead, let&amp;#8217;s introduce a bit of contextual knowledge. In our fictitious organization, the purchasing department has a price-reference table where target prices are defined for all product types. We can use this information to improve the quality of the data. Instead of training our algorithm to focus on the product type, we’ll train it to consider the target price. This way, we won&amp;#8217;t need to re-train the model when the reference price list changes.&lt;/p&gt; &lt;h3&gt;Choosing a machine learning algorithm&lt;/h3&gt; &lt;p&gt;We now have a typical classification problem: Given the incoming data, the algorithm must find a class for those data. In other words, it has to label each data item &lt;i&gt;approved&lt;/i&gt; or &lt;i&gt;denied&lt;/i&gt;. Because we have the manager’s collected responses, we can use a supervised learning method. We only need to choose the correct algorithm. The major machine learning algorithms are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Linear Regression&lt;/li&gt; &lt;li&gt;Logistic Regression&lt;/li&gt; &lt;li&gt;K-Nearest Neighbors&lt;/li&gt; &lt;li&gt;Support Vector Machines&lt;/li&gt; &lt;li&gt;Decision Trees and Random Forests&lt;/li&gt; &lt;li&gt;Neural Networks&lt;/li&gt; &lt;/ul&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: For more about each of these algorithms, see&lt;br /&gt; &lt;a target="_blank" rel="nofollow" href="https://www.freecodecamp.org/news/a-no-code-intro-to-the-9-most-important-machine-learning-algorithms-today/"&gt;9 Key Machine Learning Algorithms Explained in Plain English&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Except for linear regression, we could apply any of these algorithms to our classification problem. For this use case, we will use a Logistic Regression model. Fortunately, we don&amp;#8217;t need to understand the algorithm&amp;#8217;s implementation details. We can rely on existing tools for implementation.&lt;/p&gt; &lt;h3&gt;Python and scikit-learn&lt;/h3&gt; &lt;p&gt;We will use Python and the &lt;a target="_blank" rel="nofollow" href="https://scikit-learn.org/"&gt;scikit-learn library&lt;/a&gt; to train our Logistic Regression model. We choose Python because it is concise and easy to understand and learn. It is also the de facto standard for data scientists. Many libraries expressly designed for data science are written in Python.&lt;/p&gt; &lt;h2&gt;The example project&lt;/h2&gt; &lt;p&gt;Before we go further, download the &lt;a target="_blank" rel="nofollow" href="https://github.com/dmarrazzo/rhdm-dmn-pmml-order"&gt;project source code here&lt;/a&gt;. Open the &lt;code&gt;python&lt;/code&gt; folder to find the machine training code (&lt;code&gt;ml-training.py&lt;/code&gt;) and the CSV file we&amp;#8217;ll use to train the algorithm.&lt;/p&gt; &lt;p&gt;Even without experience with Python and machine learning, the code is easy to understand and adapt. The program&amp;#8217;s logical steps are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Initialize the algorithm to train.&lt;/li&gt; &lt;li&gt;Read the available data from a CSV file.&lt;/li&gt; &lt;li&gt;Randomly split the training and test data sets (40% is used for testing).&lt;/li&gt; &lt;li&gt;Train the model.&lt;/li&gt; &lt;li&gt;Test the model against the testing data set.&lt;/li&gt; &lt;li&gt;Print the test results.&lt;/li&gt; &lt;li&gt;Save the trained model in PMML.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;A nice feature of the &lt;code&gt;scikit-learn&lt;/code&gt; library is that its machine learning algorithms expose nearly all the same APIs. You can switch between the available algorithms by changing one line of code. This means you can easily benchmark different algorithms for accuracy and decide which one best fits your use case. This type of benchmarking is common because it&amp;#8217;s often hard to know in advance which algorithm will perform better for a use case.&lt;/p&gt; &lt;h3&gt;Run the program&lt;/h3&gt; &lt;p&gt;If you run the Python program, you should see results similar to the following, but not exactly the same. The training and test data are randomly selected so that the results will differ each time. The point is to verify that the algorithm works consistently across multiple executions.&lt;/p&gt; &lt;pre&gt;Results for model LogisticRegression Correct: 1522 Incorrect: 78 Accuracy: 95.12% True Positive Rate: 93.35% True Negative Rate: 97.10% &lt;/pre&gt; &lt;p&gt;The results are quite accurate, at 95%. More importantly, the True Negative Rate (measuring specificity) is very high, at 97.1%. In general, there is a tradeoff between the True Negative Rate and True Positive Rate, which measures sensitivity. Intuitively, you can liken the prediction sensitivity to a car alarm: If we increase an alarm&amp;#8217;s sensitivity, it is more likely to go off by mistake and increase the number of false positives. The increase in false positives lowers specificity.&lt;/p&gt; &lt;h3&gt;Tune the algorithm&lt;/h3&gt; &lt;p&gt;In this particular use case, of approving or rejecting a product order, we would reject the order. Manual approval is better than having too many false positives, which would lead to wrongly approved orders. To improve our results, we can adjust the logistic regression to reduce the prediction sensitivity.&lt;/p&gt; &lt;p&gt;Predictive machine learning models are also known as &lt;i&gt;classification&lt;/i&gt; algorithms because they place an input dataset in a specific class. In our case, we have two classes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;#8220;true&amp;#8221; to approve the order.&lt;/li&gt; &lt;li&gt;&amp;#8220;false&amp;#8221; to refuse it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To reduce the likelihood of a false positive, we can tune the &amp;#8220;true&amp;#8221; class weight (note that 1 is the default):&lt;/p&gt; &lt;pre&gt;model = LogisticRegression(class_weight ={    "true" : .6,    "false" : 1 }) &lt;/pre&gt; &lt;h3&gt;Store the model in a PMML file&lt;/h3&gt; &lt;p&gt;Python is handy for analysis, but we might prefer another language or product for running a machine learning model in production. Reasons include better performance and integration with the enterprise ecosystem.&lt;/p&gt; &lt;p&gt;What we need is a way to exchange machine learning model definitions between different software. The PMML format is commonly used for this purpose. The DMN specification includes a direct reference to a PMML model, which makes this option straightforward.&lt;/p&gt; &lt;p&gt;You should make a couple of changes to the PMML file before importing it to the DMN editor. First, you might need to change the Python PMML version tag to 4.3, which is the version supported by Decision Manager 7.7 (the current version as of this writing):&lt;/p&gt; &lt;pre&gt;&amp;#60;PMML version="4.3" xmlns="http://www.dmg.org/PMML-4_3" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&amp;#62; &lt;/pre&gt; &lt;p&gt;Next, you want to be able to easily identify the predictive model from the DMN modeler. Use the &lt;code&gt;modelName&lt;/code&gt; attribute to name your model:&lt;/p&gt; &lt;pre&gt;&amp;#60;RegressionModel modelName="approvalRegression" functionName="classification" normalizationMethod="logit"&amp;#62; &lt;/pre&gt; &lt;p&gt;The diagram in Figure 2 shows where we are currently with this project.&lt;/p&gt; &lt;div id="attachment_815657" style="width: 447px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25ef15607b.png"&gt;&lt;img aria-describedby="caption-attachment-815657" class="wp-image-815657 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25ef15607b.png" alt="The scikit-learn library requires a training set and an algorithm configuration; the outcome is the PMML model." width="437" height="147" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25ef15607b.png 437w, https://developers.redhat.com/blog/wp-content/uploads/2020/11/img_5fa25ef15607b-300x101.png 300w" sizes="(max-width: 437px) 100vw, 437px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-815657" class="wp-caption-text"&gt;Figure 2: A usage block diagram for scikit-learn.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;So far, you&amp;#8217;ve seen how to create a machine learning model and store it in a PMML file. In the second half of this article, you will learn more about using PMML to store and transfer machine learning models. You&amp;#8217;ll also discover how to consume a predictive model from a deterministic decision using DMN. Finally, we&amp;#8217;ll review the advantages of creating more cooperation between the deterministic world and the predictive one.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#38;linkname=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#38;linkname=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#38;linkname=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#38;linkname=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#38;linkname=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#38;linkname=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#38;linkname=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F14%2Fknowledge-meets-machine-learning-for-smarter-decisions-part-1%2F&amp;#038;title=Knowledge%20meets%20machine%20learning%20for%20smarter%20decisions%2C%20Part%201" data-a2a-url="https://developers.redhat.com/blog/2021/01/14/knowledge-meets-machine-learning-for-smarter-decisions-part-1/" data-a2a-title="Knowledge meets machine learning for smarter decisions, Part 1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/14/knowledge-meets-machine-learning-for-smarter-decisions-part-1/"&gt;Knowledge meets machine learning for smarter decisions, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bmbDwQ0PJ4c" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Drools is a popular open source project known for its powerful rules engine. Few users realize that it can also be a gateway to the amazing possibilities of artificial intelligence. This two-part article introduces you to using Red Hat Decision Manager and its Drools-based rules engine to combine machine learning predictions with deterministic reasoning. In [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/14/knowledge-meets-machine-learning-for-smarter-decisions-part-1/"&gt;Knowledge meets machine learning for smarter decisions, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/01/14/knowledge-meets-machine-learning-for-smarter-decisions-part-1/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">815627</post-id><dc:creator>Donato Marrazzo</dc:creator><dc:date>2021-01-14T08:00:15Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/01/14/knowledge-meets-machine-learning-for-smarter-decisions-part-1/</feedburner:origLink></entry><entry><title type="html">Using case principal transformers in Elytron</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/1V5ylAbtZzE/" /><author><name>Sonia Zaldana Calles</name></author><id>https://wildfly-security.github.io/wildfly-elytron/blog/case-principal-transformer/</id><updated>2021-01-14T00:00:00Z</updated><dc:creator>Sonia Zaldana Calles</dc:creator><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/1V5ylAbtZzE" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://wildfly-security.github.io/wildfly-elytron/blog/case-principal-transformer/</feedburner:origLink></entry><entry><title>Getting started with Tekton and Pipelines</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Epbp7VHsb-o/" /><category term="CI/CD" /><category term="Kubernetes" /><category term="Microservices" /><category term="Serverless" /><category term="Continuous Integration" /><category term="kubectl" /><category term="minikube" /><category term="openshift" /><category term="Tekton" /><author><name>Cedric Clyburn</name></author><id>https://developers.redhat.com/blog/?p=793097</id><updated>2021-01-13T08:00:28Z</updated><published>2021-01-13T08:00:28Z</published><content type="html">&lt;p&gt;Tekton is a powerful, &lt;a href="https://developers.redhat.com/blog/2020/04/08/why-kubernetes-native-instead-of-cloud-native/"&gt;Kubernetes-native&lt;/a&gt; framework for creating continuous integration and delivery (CI/CD) systems. In this article, we&amp;#8217;ll use real-world examples to show you how to install Tekton, create Tasks, and eventually create our own pipeline.&lt;/p&gt; &lt;h2&gt;What&amp;#8217;s a pipeline?&lt;/h2&gt; &lt;p&gt;Great question! In software development, pipelines are automated processes that drive software through a process of building, testing, and deploying code. Such an efficient process can help minimize human error, as well as maintain consistency in deployment. Since &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;Tekton&lt;/a&gt; is cloud-native, its pipelines are containerized and don&amp;#8217;t have dependencies on other projects, mitigating potential issues and saving you time.&lt;/p&gt; &lt;div id="attachment_846497" style="width: 634px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-846497" class=" size-full wp-image-846497 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2020/12/pipeline-concepts.png" src="https://developers.redhat.com/blog/wp-content/uploads/2020/12/pipeline-concepts.png" alt="Defining and running a pipeline from pipeline Tasks through PipelineRun TaskRuns, controllers, and pods." width="624" height="195" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/12/pipeline-concepts.png 624w, https://developers.redhat.com/blog/wp-content/uploads/2020/12/pipeline-concepts-300x94.png 300w" sizes="(max-width: 624px) 100vw, 624px" /&gt;&lt;p id="caption-attachment-846497" class="wp-caption-text"&gt;The structure of a pipeline.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;About Tekton&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/pipeline"&gt;Tekton&lt;/a&gt; is a &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Knative&lt;/a&gt;-based framework for CI/CD pipelines, but it&amp;#8217;s unique due to its decoupled nature—meaning that one pipeline can be used to deploy to any Kubernetes cluster across multiple hybrid cloud providers. In addition, Tekton stores everything related to a pipeline as &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;custom resources&lt;/a&gt; (CRs) within the cluster, allowing pieces to be used across multiple pipelines.&lt;/p&gt; &lt;h2&gt;Installing Tekton&lt;/h2&gt; &lt;p&gt;For this guide, we&amp;#8217;ll assume you&amp;#8217;re using &lt;a target="_blank" rel="nofollow" href="https://kubernetes.io/docs/tasks/tools/install-minikube/"&gt;Minikube&lt;/a&gt; for your Kubernetes cluster, although we&amp;#8217;ve created a &lt;a href="https://developers.redhat.com/courses/middleware/openshift-pipelines"&gt;Katacoda Tekton scenario&lt;/a&gt; if you don&amp;#8217;t have access to Minikube. Once your cluster is running, install the latest version of Tekton by applying the YAML from the &lt;a href="https://github.com/tektoncd/pipeline/releases"&gt;latest release&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl apply -f https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.16.3/release.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will create a &lt;code&gt;tekton-pipelines&lt;/code&gt; namespace, as well as other resources to finalize your Tekton install. With that namespace in mind, we can easily track the progress of our install using the command below:&lt;/p&gt; &lt;p&gt;&lt;code&gt;kubectl get pods --namespace tekton-pipelines --watch&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Finally, to interact with Tekton through the console, we need to install the Tekton CLI, also known as &lt;code&gt;tkn&lt;/code&gt;. Depending on your operating system, please use the instructions from the &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/cli"&gt;official repository&lt;/a&gt; to install the latest binary executable.&lt;/p&gt; &lt;h2&gt;Optional: Install the tutorial repo&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; developer advocate team has created a &lt;a target="_blank" rel="nofollow" href="https://github.com/joellord/handson-tekton"&gt;repository&lt;/a&gt; to help you get started and master Tekton concepts. If you&amp;#8217;re interested in seeing more concepts and getting hands-on, feel free to clone our repo to your local directory:&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone https://github.com/joellord/handson-tekton&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Once you&amp;#8217;ve cloned the tutorial repository, be sure to &lt;code&gt;cd&lt;/code&gt; into the folder with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;cd handson-tekton&lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Creating our first Task&lt;/h2&gt; &lt;p&gt;For our introduction to Tasks, let&amp;#8217;s start off with a simple &amp;#8220;Hello World&amp;#8221; Task. Task resources are essential building block components for creating a Pipeline, and this first Task will allow us to use a &lt;a href="https://developers.redhat.com/blog/category/ubi/"&gt;Red Hat Universal Base Image&lt;/a&gt; and echo a &amp;#8220;Hello World&amp;#8221;. To begin, let&amp;#8217;s open the file &lt;code&gt;01-hello.yaml&lt;/code&gt; in the &lt;code&gt;/demo&lt;/code&gt; folder:&lt;/p&gt; &lt;pre&gt;&lt;span class="pl-ent"&gt;apiVersion&lt;/span&gt;: &lt;span class="pl-s"&gt;tekton.dev/v1beta1&lt;/span&gt; &lt;span class="pl-ent"&gt;kind&lt;/span&gt;: &lt;span class="pl-s"&gt;Task&lt;/span&gt; &lt;span class="pl-ent"&gt;metadata&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;hello&lt;/span&gt; &lt;span class="pl-ent"&gt;spec&lt;/span&gt;: &lt;span class="pl-ent"&gt;steps&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-hello&lt;/span&gt; &lt;span class="pl-ent"&gt;image&lt;/span&gt;: &lt;span class="pl-s"&gt;registry.access.redhat.com/ubi8/ubi&lt;/span&gt; &lt;span class="pl-ent"&gt;command&lt;/span&gt;: - &lt;span class="pl-s"&gt;/bin/bash&lt;/span&gt; &lt;span class="pl-ent"&gt;args&lt;/span&gt;: &lt;span class="pl-s"&gt;['-c', 'echo Hello World']&lt;/span&gt;&lt;/pre&gt; &lt;p&gt;You&amp;#8217;ll notice several details above, from the &lt;code&gt;kind&lt;/code&gt; being a Task, to the &lt;code&gt;step&lt;/code&gt; of &amp;#8220;say-hello&amp;#8221;, and the &lt;code&gt;args&lt;/code&gt; being to simply output an &lt;code&gt;echo&lt;/code&gt; command to the console. Let&amp;#8217;s apply this Task to our cluster, similar to any other Kubernetes object:&lt;/p&gt; &lt;p&gt;&lt;code&gt;kubectl apply -f ./demo/01-hello.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tkn task start --showlog hello&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Great work! After running this &lt;code&gt;tkn&lt;/code&gt; command, you&amp;#8217;ll soon see an output from the Task in the console like such:&lt;/p&gt; &lt;pre&gt;TaskRun started: hello-run-6cgf5 Waiting for logs to be available... [say-hello] Hello World&lt;/pre&gt; &lt;h2&gt;Adding parameters to a task&lt;/h2&gt; &lt;p&gt;An important feature of Tasks is the ability to take in and pass parameters. If you&amp;#8217;re looking to build out various Pipelines, parameters, or &lt;code&gt;params&lt;/code&gt;, are instrumental. These properties are constructed of a &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt;, but can also accept a &lt;code&gt;description&lt;/code&gt; and &lt;code&gt;default&lt;/code&gt; value. To take a better look at how parameters work, let&amp;#8217;s open up the file &lt;code&gt;02-param.yaml&lt;/code&gt; in the &lt;code&gt;/demo&lt;/code&gt; folder:&lt;/p&gt; &lt;pre&gt;&lt;span class="pl-ent"&gt;apiVersion&lt;/span&gt;: &lt;span class="pl-s"&gt;tekton.dev/v1beta1&lt;/span&gt; &lt;span class="pl-ent"&gt;kind&lt;/span&gt;: &lt;span class="pl-s"&gt;Task&lt;/span&gt; &lt;span class="pl-ent"&gt;metadata&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;hello&lt;/span&gt; &lt;span class="pl-ent"&gt;spec&lt;/span&gt;: &lt;span class="pl-ent"&gt;params&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;person&lt;/span&gt; &lt;span class="pl-ent"&gt;description&lt;/span&gt;: &lt;span class="pl-s"&gt;Name of person to greet&lt;/span&gt; &lt;span class="pl-ent"&gt;default&lt;/span&gt;: &lt;span class="pl-s"&gt;World&lt;/span&gt; &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;string&lt;/span&gt; &lt;span class="pl-ent"&gt;steps&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-hello&lt;/span&gt; &lt;span class="pl-ent"&gt;image&lt;/span&gt;: &lt;span class="pl-s"&gt;registry.access.redhat.com/ubi8/ubi&lt;/span&gt; &lt;span class="pl-ent"&gt;command&lt;/span&gt;: - &lt;span class="pl-s"&gt;/bin/bash&lt;/span&gt; &lt;span class="pl-ent"&gt;args&lt;/span&gt;: &lt;span class="pl-s"&gt;['-c', 'echo Hello $(params.person)']&lt;/span&gt;&lt;/pre&gt; &lt;p&gt;Building from our &amp;#8220;Hello World&amp;#8221; example, we&amp;#8217;ve added in a &lt;code&gt;person&lt;/code&gt; parameter with generic values. In addition, to access the new param, we can call it using &lt;code&gt;$(params.person)&lt;/code&gt;. In order to run this new Task, we can add it to our cluster and run the Task with the following command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;kubectl apply -f ./demo/02-param.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tkn task start --showlog hello&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Looks good! Now, it looks as if the console is asking for us to specify the parameter in the command line, similar to below:&lt;/p&gt; &lt;pre&gt;? Value for param `person` of type `string`? (Default is `World`) Cedric TaskRun started: hello-run-z4gsw Waiting for logs to be available... [say-hello] Hello Cedric&lt;/pre&gt; &lt;h2&gt;Creating a Pipeline&lt;/h2&gt; &lt;p&gt;Now that you understand Tasks and parameters, let&amp;#8217;s dive into creating a Pipeline. For consistency, Tasks are meant for single actions, while a Pipeline is a series of Tasks that can be run either in parallel or sequentially. For this example, we&amp;#8217;ll use the &lt;code&gt;04-tasks.yaml&lt;/code&gt; file in the &lt;code&gt;/demo&lt;/code&gt; folder for our Pipeline:&lt;/p&gt; &lt;pre&gt;&lt;span class="pl-ent"&gt;apiVersion&lt;/span&gt;: &lt;span class="pl-s"&gt;tekton.dev/v1beta1&lt;/span&gt; &lt;span class="pl-ent"&gt;kind&lt;/span&gt;: &lt;span class="pl-s"&gt;Task&lt;/span&gt; &lt;span class="pl-ent"&gt;metadata&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-something&lt;/span&gt; &lt;span class="pl-ent"&gt;spec&lt;/span&gt;: &lt;span class="pl-ent"&gt;params&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-what&lt;/span&gt; &lt;span class="pl-ent"&gt;description&lt;/span&gt;: &lt;span class="pl-s"&gt;What should I say&lt;/span&gt; &lt;span class="pl-ent"&gt;default&lt;/span&gt;: &lt;span class="pl-s"&gt;hello&lt;/span&gt; &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;string&lt;/span&gt; - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;pause-duration&lt;/span&gt; &lt;span class="pl-ent"&gt;description&lt;/span&gt;: &lt;span class="pl-s"&gt;How long to wait before saying something&lt;/span&gt; &lt;span class="pl-ent"&gt;default&lt;/span&gt;: &lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-ent"&gt;type&lt;/span&gt;: &lt;span class="pl-s"&gt;string&lt;/span&gt; &lt;span class="pl-ent"&gt;steps&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-it&lt;/span&gt; &lt;span class="pl-ent"&gt;image&lt;/span&gt;: &lt;span class="pl-s"&gt;registry.access.redhat.com/ubi8/ubi&lt;/span&gt; &lt;span class="pl-ent"&gt;command&lt;/span&gt;: - &lt;span class="pl-s"&gt;/bin/bash&lt;/span&gt; &lt;span class="pl-ent"&gt;args&lt;/span&gt;: &lt;span class="pl-s"&gt;['-c', 'sleep $(params.pause-duration) &amp;#38;&amp;#38; echo $(params.say-what)']&lt;/span&gt;&lt;/pre&gt; &lt;p&gt;With this generic Task file, which will &lt;code&gt;echo&lt;/code&gt; whatever it receives in its parameters, we can build our first Pipeline. With the &lt;code&gt;05-pipeline.yaml&lt;/code&gt; file in the &lt;code&gt;/demo&lt;/code&gt; folder, we can manipulate the &lt;code&gt;04-tasks.yaml&lt;/code&gt; Task twice, with different outputs:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: say-things spec: tasks: - name: first-task params: - name: pause-duration value: "2" - name: say-what value: "Hello, this is the first task" taskRef: name: say-something - name: second-task params: - name: say-what value: "And this is the second task" taskRef: name: say-something&lt;/pre&gt; &lt;p&gt;We&amp;#8217;re now ready to apply the generic Task and the new Pipeline to our cluster, and officially start the Pipeline. Using &lt;code&gt;tkn pipeline start&lt;/code&gt;, we create a &lt;code&gt;PipelineRun&lt;/code&gt; resource automatically with a random name:&lt;/p&gt; &lt;p&gt;&lt;code&gt;kubectl apply -f ./demo/04-tasks.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;kubectl apply -f ./demo/05-pipeline.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tkn pipeline start say-things --showlog&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Congrats! You&amp;#8217;ll notice the console has to output the logs from the &lt;code&gt;PipelineRun.&lt;/code&gt; However, the order seems to be confused.&lt;/p&gt; &lt;pre&gt;PipelineRun started: say-things-run-ncfsq Waiting for logs to be available... [second-task : say-it] And this is the second task [first-task : say-it] Hello, this is the first task&lt;/pre&gt; &lt;p&gt;You&amp;#8217;ll notice that the first task seems to happen after the second task, and this is due to Tekton naturally running all the tasks simultaneously.&lt;/p&gt; &lt;h2&gt;Run in parallel or sequentially&lt;/h2&gt; &lt;p&gt;For Tasks to run in a specific order, the &lt;code&gt;runAfter&lt;/code&gt; parameter is needed in the task definition of your Pipeline. Let&amp;#8217;s open up the &lt;code&gt;06-pipeline-order.yaml&lt;/code&gt; file in the &lt;code&gt;/demo&lt;/code&gt; folder:&lt;/p&gt; &lt;pre&gt;&lt;span class="pl-ent"&gt;apiVersion&lt;/span&gt;: &lt;span class="pl-s"&gt;tekton.dev/v1beta1&lt;/span&gt; &lt;span class="pl-ent"&gt;kind&lt;/span&gt;: &lt;span class="pl-s"&gt;Pipeline&lt;/span&gt; &lt;span class="pl-ent"&gt;metadata&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-things-in-order&lt;/span&gt; &lt;span class="pl-ent"&gt;spec&lt;/span&gt;: &lt;span class="pl-ent"&gt;tasks&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;first-task&lt;/span&gt; &lt;span class="pl-ent"&gt;params&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;pause-duration&lt;/span&gt; &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-what&lt;/span&gt; &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Hello, this is the first task&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-ent"&gt;taskRef&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-something&lt;/span&gt; - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;second-task&lt;/span&gt; &lt;span class="pl-ent"&gt;params&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-what&lt;/span&gt; &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Happening after task 1, in parallel with task 3&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;pause-duration&lt;/span&gt; &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-ent"&gt;taskRef&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-something&lt;/span&gt; &lt;span class="pl-ent"&gt;runAfter&lt;/span&gt;: - &lt;span class="pl-s"&gt;first-task&lt;/span&gt; - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;third-task&lt;/span&gt; &lt;span class="pl-ent"&gt;params&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-what&lt;/span&gt; &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Happening after task 1, in parallel with task 2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;pause-duration&lt;/span&gt; &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-ent"&gt;taskRef&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-something&lt;/span&gt; &lt;span class="pl-ent"&gt;runAfter&lt;/span&gt;: - &lt;span class="pl-s"&gt;first-task&lt;/span&gt; - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;fourth-task&lt;/span&gt; &lt;span class="pl-ent"&gt;params&lt;/span&gt;: - &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-what&lt;/span&gt; &lt;span class="pl-ent"&gt;value&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Happening after task 2 and 3&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-ent"&gt;taskRef&lt;/span&gt;: &lt;span class="pl-ent"&gt;name&lt;/span&gt;: &lt;span class="pl-s"&gt;say-something&lt;/span&gt; &lt;span class="pl-ent"&gt;runAfter&lt;/span&gt;: - &lt;span class="pl-s"&gt;second-task&lt;/span&gt; - &lt;span class="pl-s"&gt;third-task&lt;/span&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;runAfter&lt;/code&gt; parameter is being applied to specific numbered tasks, and after applying this Pipeline to our cluster, we&amp;#8217;ll be able to see logs from each task, but ordered:&lt;/p&gt; &lt;p&gt;&lt;code&gt;kubectl apply -f ./demo/06-pipeline-order.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;tkn pipeline start say-things-in-order --showlog&lt;/code&gt;&lt;/p&gt; &lt;p&gt;After running &lt;code&gt;tkn&lt;/code&gt;, your CLI output should be similar to this example:&lt;/p&gt; &lt;pre&gt;PipelineRun started: say-things-in-order-run-5dklz Waiting for logs to be available... [first-task : say-it] Hello, this is the first task [second-task : say-it] Happening after task 1, in parallel with task 3 [third-task : say-it] Happening after task 1, in parallel with task 2 [fourth-task : say-it] Happening after task 2 and 3&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Feel free to &lt;a target="_blank" rel="nofollow" href="https://github.com/joellord/handson-tekton"&gt;continue the demo here&lt;/a&gt;, and try out our &lt;a href="https://developers.redhat.com/courses/middleware/openshift-pipelines"&gt;guided Katacoda scenario here&lt;/a&gt; as well, which offers an interactive environment right in your browser.&lt;/p&gt; &lt;p&gt;For more interactive demonstrations of many of the examples you&amp;#8217;ve seen here, check out our video!&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/pEmyyrjLrBE?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;Resources&lt;/h2&gt; &lt;p&gt;If you want to keep learning about Tekton, start with these articles on Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/08/14/introduction-to-cloud-native-ci-cd-with-tekton-kubecon-europe-2020/"&gt;Introduction to cloud-native CI/CD with Tekton&lt;/a&gt; (Jan Kleinert &amp;#38; Joel Lord)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/01/08/the-new-tekton-pipelines-extension-for-visual-studio-code/"&gt;The new Tekton Pipelines extension for Visual Studio Code&lt;/a&gt; (Denis Golovin &amp;#38; Lindsey Tulloch)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/04/30/creating-pipelines-with-openshift-4-4s-new-pipeline-builder-and-tekton-pipelines/"&gt;Creating Pipelines with OpenShift 4.4’s new Pipeline Builder and Tekton Pipelines&lt;/a&gt; (Joel Lord)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#38;linkname=Getting%20started%20with%20Tekton%20and%20Pipelines" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#38;linkname=Getting%20started%20with%20Tekton%20and%20Pipelines" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#38;linkname=Getting%20started%20with%20Tekton%20and%20Pipelines" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#38;linkname=Getting%20started%20with%20Tekton%20and%20Pipelines" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#38;linkname=Getting%20started%20with%20Tekton%20and%20Pipelines" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#38;linkname=Getting%20started%20with%20Tekton%20and%20Pipelines" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#38;linkname=Getting%20started%20with%20Tekton%20and%20Pipelines" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F01%2F13%2Fgetting-started-with-tekton-and-pipelines%2F&amp;#038;title=Getting%20started%20with%20Tekton%20and%20Pipelines" data-a2a-url="https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines/" data-a2a-title="Getting started with Tekton and Pipelines"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines/"&gt;Getting started with Tekton and Pipelines&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Epbp7VHsb-o" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Tekton is a powerful, Kubernetes-native framework for creating continuous integration and delivery (CI/CD) systems. In this article, we&amp;#8217;ll use real-world examples to show you how to install Tekton, create Tasks, and eventually create our own pipeline. What&amp;#8217;s a pipeline? Great question! In software development, pipelines are automated processes that drive software through a process of [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines/"&gt;Getting started with Tekton and Pipelines&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">3</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">793097</post-id><dc:creator>Cedric Clyburn</dc:creator><dc:date>2021-01-13T08:00:28Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines/</feedburner:origLink></entry></feed>
